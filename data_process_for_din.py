import pandas as pd
import numpy as np
import torch
import pickle
from pathlib import Path
from collections import deque
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from typing import Dict, List, Set, Generator
from tqdm import tqdm

## ----------------- 1. é…ç½® ----------------- ##

# --- è¾“å…¥/è¾“å‡ºæ–‡ä»¶è·¯å¾„ ---
DATA_DIR = Path('Data')
PROCESSED_DATA_DIR = Path('Processed_Data')
# æ–°å¢ä¸€ä¸ªä¸“é—¨å­˜æ”¾åˆ†å—æ–‡ä»¶çš„ç›®å½•
SUBMISSION_CHUNKS_DIR = PROCESSED_DATA_DIR / 'submission_chunks'
PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)
SUBMISSION_CHUNKS_DIR.mkdir(parents=True, exist_ok=True) # åˆ›å»ºåˆ†å—ç›®å½•

CSV_FILE_PATH = DATA_DIR / 'train.csv'
SUBMISSION_CSV_PATH = DATA_DIR / 'submission.csv'
TRAIN_DF_PATH = PROCESSED_DATA_DIR / 'train_df.pkl'
TEST_DF_PATH = PROCESSED_DATA_DIR / 'test_df.pkl'
# æ³¨æ„ï¼šæœ€ç»ˆçš„submissionæ–‡ä»¶å°†ç”±å¤šä¸ªå—ç»„æˆï¼Œè¿™é‡Œä¸å†å®šä¹‰å•ä¸€çš„è¾“å‡ºè·¯å¾„
# SUBMISSION_DF_PATH = PROCESSED_DATA_DIR / 'submission_df.pkl'
ENCODERS_PATH = PROCESSED_DATA_DIR / 'label_encoders.pkl'
VOCAB_SIZES_PATH = PROCESSED_DATA_DIR / 'vocab_sizes.pkl'

# --- åŸºäºæ—¶é—´çš„åˆ†å‰²é…ç½® ---
TRAIN_TARGET_MONTH = '2013-07'
TEST_TARGET_MONTH = '2013-08'
PREDICT_TARGET_MONTH = '2013-09'

# --- æ¨¡å‹è¶…å‚æ•° ---
MAX_SEQ_LENGTH = 50
BATCH_SIZE = 128
NEG_SAMPLES_PER_POS = 4
# æ–°å¢ï¼šç”¨äºåˆ†æ‰¹ä¿å­˜çš„æ‰¹æ¬¡å¤§å°ï¼ˆæ¯ä¸ªæ–‡ä»¶åŒ…å«çš„ç”¨æˆ·æ•°ï¼‰
SUBMISSION_CHUNK_SIZE = 1000 # æ¯ä¸ªåˆ†å—æ–‡ä»¶åŒ…å«1000ä¸ªç”¨æˆ·çš„é¢„æµ‹æ ·æœ¬

## ----------------- 2. PyTorch Dataset å’Œ Collate å‡½æ•° (æ— å˜åŠ¨) ----------------- ##
def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
    """
    è‡ªå®šä¹‰çš„collateå‡½æ•°ï¼Œç”¨äºå¤„ç†å˜é•¿åºåˆ—ã€‚
    """
    batch_dict = {key: [d[key] for d in batch] for key in batch[0]}
    batch_dict['history_goods'] = pad_sequence(batch_dict['history_goods'], batch_first=True, padding_value=0)
    batch_dict['history_classes'] = pad_sequence(batch_dict['history_classes'], batch_first=True, padding_value=0)

    for key in batch_dict:
        if key not in ['history_goods', 'history_classes']:
            batch_dict[key] = torch.stack(batch_dict[key], dim=0)
    return batch_dict

## ----------------- 3. æ•°æ®å¤„ç†æ ¸å¿ƒå‡½æ•° ----------------- ##

def create_din_samples(df: pd.DataFrame, target_month: str, all_goods_indices: Set[int], goods_to_class_map: Dict[int, int]) -> pd.DataFrame:
    """
    ä¸ºç»™å®šçš„ç›®æ ‡æœˆä»½åˆ›å»ºDINæ¨¡å‹çš„è®­ç»ƒ/æµ‹è¯•æ ·æœ¬ã€‚(æ­¤å‡½æ•°æ— å˜åŠ¨)
    """
    print(f"æ­£åœ¨ä¸ºç›®æ ‡æœˆä»½ '{target_month}' ç”Ÿæˆæ ·æœ¬...")
    history_df = df[df['month'] < target_month]
    target_df = df[df['month'] == target_month]

    user_history = history_df.groupby('customer_id_idx').agg({
        'goods_id_idx': list,
        'goods_class_id_idx': list
    }).reset_index()
    user_history.rename(columns={'goods_id_idx': 'history_goods', 'goods_class_id_idx': 'history_classes'}, inplace=True)

    positive_samples = target_df.groupby('customer_id_idx')['goods_id_idx'].apply(list).reset_index(name='target_goods')
    merged_df = pd.merge(positive_samples, user_history, on='customer_id_idx', how='left')
    merged_df['history_goods'] = merged_df['history_goods'].apply(lambda x: x if isinstance(x, list) else [])
    merged_df['history_classes'] = merged_df['history_classes'].apply(lambda x: x if isinstance(x, list) else [])

    din_samples = []
    for _, row in tqdm(merged_df.iterrows(), total=len(merged_df), desc="Generating train/test samples"):
        history_g = list(deque(row['history_goods'], maxlen=MAX_SEQ_LENGTH))
        history_c = list(deque(row['history_classes'], maxlen=MAX_SEQ_LENGTH))
        
        for good_idx in row['target_goods']:
            din_samples.append({
                'customer_id': row['customer_id_idx'],
                'history_goods': history_g,
                'history_classes': history_c,
                'candidate_good': good_idx,
                'candidate_class': goods_to_class_map.get(good_idx, 0),
                'label': 1
            })

        purchased_set = set(row['target_goods'])
        negative_pool = list(all_goods_indices - purchased_set)
        num_neg_to_sample = min(len(negative_pool), len(purchased_set) * NEG_SAMPLES_PER_POS)
        if num_neg_to_sample > 0:
            neg_good_indices = np.random.choice(negative_pool, size=num_neg_to_sample, replace=False)
            for neg_good_idx in neg_good_indices:
                din_samples.append({
                    'customer_id': row['customer_id_idx'],
                    'history_goods': history_g,
                    'history_classes': history_c,
                    'candidate_good': neg_good_idx,
                    'candidate_class': goods_to_class_map.get(neg_good_idx, 0),
                    'label': 0
                })
    return pd.DataFrame(din_samples)

# ==================== ä¸»è¦ä¿®æ”¹çš„å‡½æ•° ====================
def create_and_save_submission_chunks(
    df: pd.DataFrame, 
    target_customers_idx: List[int], 
    all_goods_indices: Set[int], 
    goods_to_class_map: Dict[int, int],
    chunk_size: int,
    output_dir: Path
):
    """
    ä¿®æ”¹åçš„å‡½æ•°ï¼šä¸ºsubmissionç”¨æˆ·ç”Ÿæˆæ ·æœ¬ï¼Œå¹¶åˆ†å—ä¿å­˜ä¸ºpickleæ–‡ä»¶ï¼Œä»¥é˜²æ­¢å†…å­˜çˆ†ç‚¸ã€‚
    """
    print(f"ğŸš€ å¼€å§‹ä¸ºç›®æ ‡æœˆä»½ '{PREDICT_TARGET_MONTH}' çš„é¢„æµ‹ä»»åŠ¡ç”Ÿæˆæ ·æœ¬ï¼ˆåˆ†å—æ¨¡å¼ï¼‰...")
    
    # 1. å‡†å¤‡ç”¨æˆ·å†å²æ•°æ®
    history_df = df[df['month'] < PREDICT_TARGET_MONTH]
    user_history_map = history_df.groupby('customer_id_idx').agg({
        'goods_id_idx': list,
        'goods_class_id_idx': list
    }).to_dict('index')

    all_goods_list = list(all_goods_indices)
    
    chunk_samples = []
    chunk_num = 0
    total_customers = len(target_customers_idx)

    # 2. éå†ç›®æ ‡ç”¨æˆ·ï¼Œåˆ†å—å¤„ç†
    for i, customer_idx in enumerate(tqdm(target_customers_idx, desc="ä¸ºæ‰€æœ‰ç”¨æˆ·ç”Ÿæˆå¹¶ä¿å­˜é¢„æµ‹æ ·æœ¬")):
        
        user_data = user_history_map.get(customer_idx, {})
        history_g = list(deque(user_data.get('goods_id_idx', []), maxlen=MAX_SEQ_LENGTH))
        history_c = list(deque(user_data.get('goods_class_id_idx', []), maxlen=MAX_SEQ_LENGTH))
        
        # 3. ä¸ºå•ä¸ªç”¨æˆ·å’Œæ‰€æœ‰å•†å“åˆ›å»ºæ ·æœ¬
        for good_idx in all_goods_list:
            chunk_samples.append({
                'customer_id': customer_idx,
                'history_goods': history_g,
                'history_classes': history_c,
                'candidate_good': good_idx,
                'candidate_class': goods_to_class_map.get(good_idx, 0),
                'label': 0  # å ä½ç¬¦
            })
        
        # 4. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°åˆ†å—å¤§å°æˆ–æ˜¯æœ€åä¸€ä¸ªç”¨æˆ·
        # å½“å¤„ç†çš„ç”¨æˆ·æ•°è¾¾åˆ°chunk_sizeæˆ–è€…è¿™æ˜¯æœ€åä¸€ä¸ªç”¨æˆ·æ—¶ï¼Œä¿å­˜å½“å‰å—
        if (i + 1) % chunk_size == 0 or (i + 1) == total_customers:
            chunk_df = pd.DataFrame(chunk_samples)
            chunk_path = output_dir / f'submission_chunk_{chunk_num}.pkl'
            chunk_df.to_pickle(chunk_path)
            
            print(f"âœ… å·²ä¿å­˜åˆ†å— {chunk_num} åˆ° '{chunk_path}'. æ ·æœ¬æ•°: {len(chunk_df)}")
            
            # é‡ç½®åˆ—è¡¨å’Œè®¡æ•°å™¨
            chunk_samples = []
            chunk_num += 1
            
    print(f"\nğŸ‰ æ‰€æœ‰é¢„æµ‹æ ·æœ¬å·²åˆ†å—ä¿å­˜åœ¨ç›®å½•: '{output_dir}'")

## ----------------- 4. ä¸»æ‰§è¡Œé€»è¾‘ ----------------- ##
def main():
    """ä¸»é¢„å¤„ç†æµç¨‹"""
    print("ğŸš€ å¼€å§‹DINæ•°æ®é¢„å¤„ç†...")

    # 1. åŠ è½½å’Œåˆæ­¥æ¸…æ´—æ•°æ®
    df = pd.read_csv(CSV_FILE_PATH)
    df['order_pay_time'] = pd.to_datetime(df['order_pay_time'], errors='coerce')
    df.dropna(subset=['customer_id', 'goods_id', 'goods_class_id', 'order_pay_time'], inplace=True)
    df['month'] = df['order_pay_time'].dt.strftime('%Y-%m')
    df.sort_values(by='order_pay_time', inplace=True)
    df.reset_index(drop=True, inplace=True)
    print("âœ… æ•°æ®åŠ è½½ã€æ¸…æ´—å¹¶æŒ‰æ—¶é—´æ’åºå®Œæˆã€‚")

    # 2. ç‰¹å¾ç¼–ç 
    customer_encoder = LabelEncoder()
    goods_encoder = LabelEncoder()
    class_encoder = LabelEncoder()

    df['customer_id_idx'] = customer_encoder.fit_transform(df['customer_id']) + 1
    df['goods_id_idx'] = goods_encoder.fit_transform(df['goods_id']) + 1
    df['goods_class_id_idx'] = class_encoder.fit_transform(df['goods_class_id']) + 1
    print("âœ… ç”¨æˆ·ã€å•†å“å’Œç±»åˆ«IDç¼–ç å®Œæˆã€‚")

    # 3. ä¿å­˜ç¼–ç å™¨å’Œè¯æ±‡è¡¨å¤§å°
    encoders = {
        'customer': customer_encoder, 'goods': goods_encoder, 'class': class_encoder
    }
    with open(ENCODERS_PATH, 'wb') as f: pickle.dump(encoders, f)
    print(f"âœ… ç¼–ç å™¨å·²ä¿å­˜åˆ°: '{ENCODERS_PATH}'")
    
    vocab_sizes = {
        'n_customers': len(customer_encoder.classes_) + 1,
        'n_goods': len(goods_encoder.classes_) + 1,
        'n_classes': len(class_encoder.classes_) + 1
    }
    with open(VOCAB_SIZES_PATH, 'wb') as f: pickle.dump(vocab_sizes, f)
    print(f"è¯æ±‡è¡¨å¤§å°: {vocab_sizes}")
    
    # 4. å‡†å¤‡ç”Ÿæˆæ ·æœ¬æ‰€éœ€çš„æ•°æ®ç»“æ„
    goods_to_class_map = dict(zip(df['goods_id_idx'], df['goods_class_id_idx']))
    all_goods = set(df['goods_id_idx'].unique())
    
    # 5. ç”Ÿæˆå¹¶ä¿å­˜è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
    print("\nğŸ› ï¸ æ­£åœ¨ç”Ÿæˆè®­ç»ƒé›†...")
    train_df = create_din_samples(df, TRAIN_TARGET_MONTH, all_goods, goods_to_class_map)
    train_df.to_pickle(TRAIN_DF_PATH)
    print(f"âœ… è®­ç»ƒé›†å·²ä¿å­˜åˆ° '{TRAIN_DF_PATH}'. æ ·æœ¬æ•°: {len(train_df)}")

    print("\nğŸ› ï¸ æ­£åœ¨ç”Ÿæˆæµ‹è¯•é›†...")
    test_df = create_din_samples(df, TEST_TARGET_MONTH, all_goods, goods_to_class_map)
    test_df.to_pickle(TEST_DF_PATH)
    print(f"âœ… æµ‹è¯•é›†å·²ä¿å­˜åˆ° '{TEST_DF_PATH}'. æ ·æœ¬æ•°: {len(test_df)}")

    # ==================== ä¿®æ”¹åçš„è°ƒç”¨é€»è¾‘ ====================
    # 6. ç”Ÿæˆå¹¶åˆ†å—ä¿å­˜é¢„æµ‹ç”¨æ•°æ®
    print("\nğŸ› ï¸ æ­£åœ¨ç”Ÿæˆ Submission Set (åˆ†å—æ¨¡å¼)...")
    submission_df_raw = pd.read_csv(SUBMISSION_CSV_PATH)
    known_customers = set(customer_encoder.classes_)
    submission_customers_known = submission_df_raw[submission_df_raw['customer_id'].isin(known_customers)]
    target_customers_idx = customer_encoder.transform(submission_customers_known['customer_id']) + 1
    
    # æ¸…ç†ä¸€ä¸‹åˆ†å—ç›®å½•ï¼Œé˜²æ­¢æ—§æ–‡ä»¶å¹²æ‰°
    for old_chunk in SUBMISSION_CHUNKS_DIR.glob('*.pkl'):
        old_chunk.unlink()

    # è°ƒç”¨æ–°çš„åˆ†å—ä¿å­˜å‡½æ•°
    create_and_save_submission_chunks(
        df=df,
        target_customers_idx=target_customers_idx,
        all_goods_indices=all_goods,
        goods_to_class_map=goods_to_class_map,
        chunk_size=SUBMISSION_CHUNK_SIZE,
        output_dir=SUBMISSION_CHUNKS_DIR
    )
    # ==========================================================

    print("\nğŸ‰ é¢„å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()