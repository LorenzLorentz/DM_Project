**NOTE: 先用md写, 再用latex排版**

## 2025春数据挖掘课程项目 -- 用户购买预测

### 1. 数据分析与预处理

我们首先观察整个数据集的结构

```
order_detail_id,
order_id,
order_total_num,
order_amount,
order_total_payment,
order_total_discount,
order_pay_time,
order_status,
order_count,
is_customer_rate,
order_detail_status,
order_detail_goods_num,
order_detail_amount,
order_detail_payment,
order_detail_discount,
customer_province,
customer_city,
member_id,
customer_id,
customer_gender,
member_status,
is_member_actived,
goods_id,
goods_class_id,
goods_price,
goods_status,
goods_has_discount,
goods_list_time,
goods_delist_time
```

我们拥有的是大量的用户交易记录, 我们要从这些孤立的订单中学习用户的行为模式, 从而预测下一个月的购买行为. 为了实现对用户购买行为的建模, 我们从两个角度出发进行特征工程.

首先, 我们可以假设用户的长期习惯和总体特征是决定其未来行为的关键, 从而为每个用户创建一个**静态但全面**的个人档案. 我们从以下几个特征大类出发, 试图回答这样的问题: "这位用户总的来说是一位怎样的顾客?". 是一位高价值用户, 还是一位价格敏感型用户, 还是一位冲动型消费者?

- 最后一次购买的局部信息: 我们捕捉了用户最后一次购买的几乎所有信息, 比如最后购买的商品 (`goods_id_last`), 最后的订单状态 (`order_status_last`), 最后的支付时间 (`order_pay_time_last`) 等. 这可以描述用户**最近的状态**.
    
- 所有时间的总体信息: 我们计算了用户历史上的总购买次数 (`order_count`), 总消费金额 (`order_total_payment_sum`), 平均客单价 (`order_total_payment_mean`), 价格偏好的波动 (`goods_price_std`) 等. 这描绘了用户的**长期行为模式和消费能力**。
    
- 衍生特征: 我们创造了一些新的指标，例如折扣敏感度 (`discount`) 和商品上架时长 (`goods_time_diff`)，试图从更深层次理解用户的偏好。

另一方面, 我们还可以将用户行为视作一个连续的时间序列, 通过捕捉近期的行为趋势去进行预测下个月的行为. 我们希望借助这些特征去捕捉, "这位用户最近在做什么?", "他的消费是正在升温还是正在冷却?", "他的购买行为是持续性的还是脉冲式的?", 这些时序上的模式.

- 实现方式:
    * 滑动窗口: 我们在不同的时间窗口(如过去14天、30天、91天)内分析用户的行为. (注意: 此处三个时间窗口大小不能互相整除, 因为我们要避免周期性, 捕捉更加复杂的行为模式)
    
    * 趋势和新近度指标: 我们计算了诸如 "过去14天总消费" (`payment_sum_14d`), "过去30天内有几天在购物" (`payment_sales_days_in_last_30d`), "上次购物在多少天前" (`payment_last_sale_days_ago_30d`) 等.
    
    * 衰减加权: 更进一步, 我们认为近期行为更加重要, 在计算均值时给予了更高权重。

#### **第三幕：未来的蓝图 (The Detailed Path Forward)**

基于以上的探索，我们不仅找到了一个有效的方向，更重要的是获得了宝贵的洞见。下一步，我们可以融合两种方法的优点，并进行更深层次的挖掘。

**核心思路：以“用户画像”为骨架，以“动态行为”为血肉，构建一个更强大的预测模型。**

**更Detail的想法：**

1.  **融合两种策略的精华——构建RFM+特征体系：**
    * **Recency (近度):** 保留Code 1中的 `order_pay_time_last_diff`，这是最强的Recency特征。可以从Code 2中借鉴 `payment_last_sale_days_ago_91d` 作为补充。
    * **Frequency (频度):** Code 1的 `order_count` 是很好的全局频度。我们可以增加一些动态频度特征，比如“过去90天内的购买次数”、“平均购买间隔天数” (`(last_order_time - first_order_time) / order_count`)。
    * **Monetary (额度):** Code 1的 `order_total_payment_sum` 和 `_mean` 已经很好了。可以增加“最高单笔订单金额”、“过去90天消费总额”等作为补充。

2.  **深挖商品维度特征 (Product-level Features):**
    * **品类偏好度：** 用户是“专一型”还是“探索型”？计算每个用户购买过的独立商品品类数量 (`goods_class_id` 的 `nunique`)。
    * **复购之王：** 用户最常购买的`goods_id`或`goods_class_id`是什么？这可以作为用户的“本命商品/品类”。
    * **价格带偏好：** 用户是偏爱高价商品还是低价商品？计算用户购买商品的平均 `goods_price`。

3.  **时间序列特征的精炼提取 (Smarter Time-series Features):**
    * **购买周期性：** 用户是否习惯在周末购物？（`order_pay_time_last_weekday` 是个好的开始，可以扩展到统计周末订单比例）。用户是否是“发薪日”（月初/月中）剁手党？（统计月初/月中/月末的订单比例）。
    * **活跃度变化：** 计算用户“最近3次购买的平均间隔”与“所有购买历史的平均间隔”的比值，来判断其活跃度是在上升还是下降。

4.  **用户关系与状态特征 (Customer Relationship Features):**
    * **会员生命周期：** 用户成为会员 (`is_member_actived`) 了多久？会员状态的变化情况（例如，从非激活到激活）。
    * **互动积极性：** 用户评价订单的比例 (`is_customer_rate_mean`) 是一个极佳的互动指标。爱评价的用户通常更投入。

### 2. 算法实现与分析

我们考虑使用mlp, lgb, xgb三种算法

XGBoost会先建立一棵简单的决策树进行初步判断, 接着针对这些错误, 再打造一棵新的树, 采用层序遍历的方式构建决策树.

* **他的哲学：** 抓住主要矛盾，追求极致效率。LightGBM认为，不是所有的错误都同等重要。他会优先去修正那些“错得最离谱”的样本，因为纠正这些样本能带来最大的收益。
* **他的技艺：** 他独创了“**叶序遍历（Leaf-wise）**”的绝技。他不像XGBoost那样一层一层地平均用力，而是会找到当前“最有潜力”的一片叶子，集中所有功力让其继续分裂生长。这就像一位顶级的棋手，会直捣黄龙，攻击对方最薄弱的环节。此外，他还拥有特征分桶（Histogram-based）等加速技术，让他的“铸剑”速度远超常人。
* **在我们的项目中：** LightGBM是一位天才少年剑客，出招快、准、狠。他能以比XGBoost快数倍的速度完成训练，并且往往能找到通往正确答案的捷径，直击问题的核心。

##### **神匠三：MLP - “抽象派的艺术家” (The Abstract Artist)**

* **他的哲学：** 万物皆可相连，规律隐藏于无形。MLP（神经网络）的思维方式与前两位截然不同。他不依赖于“如果-那么”（if-then）的显式规则，而是通过构建一个复杂的神经元网络，让数据在其中流动和交织。每一层神经元都会对输入的信息进行一次抽象和重组，试图从更高维度、更抽象的层面去理解用户的行为模式。
* **他的技艺：** 他通过“**非线性变换**”和“**权重学习**”来捕捉特征间极其复杂的相互作用。比如，他可能发现“用户所在城市”、“平均购买间隔”和“商品品类偏好”这三个看似无关的特征组合在一起时，会产生一种奇特的、难以言喻的预测信号。
* **在我们的项目中：** MLP是一位前卫的艺术家，他不会告诉你他是因为“用户总消费额高”而做出判断。他会告诉你，他是从你所有特征组合而成的一幅“印象派画作”中，感受到了强烈的“购买”气息。他的潜力上限极高，但也最难捉摸。

---

#### **第六幕：预言与终局 (Predicted Outcome & The Final Strategy)**

##### **效果预测与原因分析**

1.  **冠军争夺者：LightGBM 与 XGBoost**
    * **可能的效果：** 这两者将大概率成为表现最好的模型，在AUC、F1-score等关键指标上并驾齐驱，难分伯仲。**LightGBM可能会以微弱的优势胜出，并且训练速度会显著快于XGBoost。**
    * **原因分析：** 我们的数据是**结构化表格数据 (Tabular Data)**，并且我们已经通过特征工程（上一章的故事）手动创建了大量有业务含义的强特征。**梯度提升决策树（GBDT）家族（XGBoost和LGBM）是处理这类数据的王者。** 它们能非常有效地处理数值型特征和类别特征，并且能清晰地捕捉到我们创造的“总和”、“均值”、“最后一次”等特征的价值。LightGBM的速度优势来自于其算法设计上的优化，而其效果优势则可能来自于Leaf-wise生长策略在寻找最优分割点上的高效性。

2.  **潜力黑马：MLP**
    * **可能的效果：** MLP的表现存在较大的不确定性，**很可能在未经深度调优的情况下，表现不及LightGBM和XGBoost。**
    * **原因分析：**
        * **特征工程的“双刃剑”：** 我们强大的特征工程已经把很多线性的、简单的非线性关系提炼出来了。这对于树模型是“喂到嘴边”，但对于MLP，可能反而限制了它自由探索更深层、更诡异的非线性关系的能力。MLP更擅长从“原始”数据中自己发现关系。
        * **数据敏感性：** MLP对数据缩放（Scaling）非常敏感，需要对所有特征进行归一化或标准化处理，否则训练过程可能不稳定。
        * **调参难度：** MLP的超参数空间巨大（网络层数、神经元个数、激活函数、优化器、学习率等），找到最优组合需要大量的实验和经验。
        * **“黑箱”特性：** 我们很难解释MLP为什么会做出某个具体的预测，这在需要向业务方解释模型逻辑时是一个劣势。

##### **最终的制胜策略：集成学习 (Ensemble Learning)**

故事的最高潮，往往不是选出一位孤胆英雄，而是组建一支“复仇者联盟”。

**最终方案：** 我们不会在三位神匠中“三选一”，而是**博采众长，融三者为一体**。

1.  **初步赛：** 分别独立训练和精细调优一个LightGBM模型和一个XGBoost模型。同时，也构建一个经过审慎设计的MLP模型。
2.  **决赛圈 - Stacking/Blending：** 我们将这三个模型的预测结果（例如，每个用户购买的概率）作为**新的特征**，再输入到一个更简单的最终模型（如逻辑回归）中，由它来做最终的“裁决”。或者，直接对这三个模型的预测概率进行加权平均。

**为什么这是最佳策略？**
因为三位神匠的“思维方式”各不相同：
* LightGBM和XGBoost虽然同属一脉，但其建树策略的差异会让它们在某些样本上的判断出现细微偏差。
* MLP则从一个完全不同的维度（抽象非线性关系）来看待问题。

通过集成，我们可以**综合不同模型的视角，纠正彼此的偏见，从而得到一个比任何单一模型都更稳健、更准确的终极预测模型。** 这就像让稳重的大师、迅捷的剑客和天马行空的艺术家共同会诊，最终得出的结论，无疑是最接近真相的。

- 特征工程 & 不同特征的影响

- 至少实现3种算法（可以调包）& 分析不同算法差异

- 自己手动实现1种算法

- 自己划分训练集和验证集 & 汇报验证集上的结果即可